 Part2. 생성형 AI와 LLM

# 딥러닝 등의 머신러닝은 인공지능의 일부일 뿐, 인공지능 전체는 아닙니다.
- Marvin L. Minsky, Father of AI(1927~2016)

# 최근 10년 간의 AI 흐름
2012 AlexNet - 개고양이 분류
2016 AlphaGo
2018 GPT
2020 GPT3
2022 ChatGPT(GPT3.5)
2023 GPT4

# 생성형 AI의 발전속도는 사람들의 애초 예상보다 훨씬 빠르게 진행되고 있다.

# 우리가 ChatGPT에 열광하는 이유는 다음과 같음.
성능과 속도, 즉 생산성, 효율성, 창의성이 뛰어나다.

# GPT4는 변호사 시험에서 상위 10%의 성적을 거둠.
GPT3.5의 하위 10% 성적에 비해 괄목할만한 성장

# 미드저니는 미술대회에서 1등을 차지했다.
제이슨 앨런은 미드저니를 통해 생성한 '스페이스 오페라 극장'을 출품하여 2022년 열린 '콜로라도 주립박람회 미술대회' 디지털 아트 부문 1등을 차지했습니다.
우승자 앨런은 대회에 출품한 3개의 작품을 위해 80시간을 투자했다고 합니다.

# GPT : 대규모 언어 모델 기반의 대화형 챗봇. OpenAI에서 2018년 비영리 목적으로 개발을 시작
Generative Pre-trained Transformer : 생성하는 + 미리 학습된 + Transformer라는 AI 언어학습 기술

# GPT는 다음 단어를 맞추는 함수 : LM(Language Model)
챗봇은 구글 검색어 자동완성처럼 다음 단어의 확률을 맞추는 확률 모델이다.

# GPT 모델 파라미터 발전 속도는 다음과 같습니다.
GPT 파라미터 수는 2018년 GPT1일 떄 1.17억개, 2019년 GPT2일때 15억개, 2020년 GPT3일 때 1750억개가 되었다.

# NLP's Moore's Law 에 따르면 매년 10배씩 언어 모델 사이즈가 커지는 것으로 볼 수 있다.

# 생성형 AI 경쟁 타임라인을 보면 ChatGPT 출시 이후 빅테크 기업들은 경쟁적으로 대규모 모델과 서비스를 출시 중이다.
Gemini는 구글이 23.12 발표한 LaMDA 및 PaLM 2의 후속으로 구글 딥마인드에서 개발한 다중 모드 대형 언어 모델 제품군이다.
Anthropic은 2021년 1월 창업한 미국의 인공지능 스타트업이다. 창업자 전원이 OpenAI 출신의 인물이며, 생성형 인공지능 업체 중에서는 OpenAI에 이어서 규모가 가장 큰 기업이다.
OpenAI가 마이크로소프트의 투자를 받으며 영리화되자, 의견 충돌로 인해 차례로 퇴사하고 설립했다고 한다. 따라서 Anthropic은 공익기업을 표방한다. 
언어 모델 Claude-3는 2024년 2월에 런칭되었다. GPT-4보다 앞선 성능을 보인다고 발표했다.
작문과 연관된 능력, 다국어에 대한 이해도가 출시 당시 기준 다른 언어 모델보다 뛰어난 편이다. 특히 유료 버전의 기계 번역의 성능이 속도를 제외하면 정확성 측면에서 매우 뛰어나다. 번역될 결과물의 문체도 지정할 수 있고, 특정 지식을 고려하게 하는 언어 모델 기반 번역에 대해서는 기계 번역 문서를 참고할 수 있다. 한국어도 지원되며, GPT4가 지원하는 모든 언어, 심지어 한문(전문적인 주제를 다루는 동양 고전도 어느 정도 해석이 된다.)

# 생성형 AI 모델 성능 비교 시 사용될 수 있는 성능지표는 다음과 같다.
MMLU: 다중 선택 질문으로 구성된 학술 테스트
GPQA : 고급 논리적 추론과 문제 해결 능력 평가
GSM8K: 초등학교 수학 문제
MATH: 다양한 난이도와 하위 범주로 구성된 수학
MGSM : 여러 언어로 된 초등학교 수준의 수학 문제
HumanEval: Python 코딩 과제
DROP: 독해력과 산수 능력 평가
BIG-Bench-Hard: 복잡하고 도전적인 문제들로 구성
ARC-Challenge : 과학 문제에 대한 능력 평가
HellaSwag: 일반 상식 및 문장 완성 능력 테스트
NaturalCode: 새로운 코드 생성 평가.
WMT23: 기계 번역 평가.

Claude 3와 GPT, Gemini 모델 간 성능 격차는 그리 크지 않다.

# Hype Cycle for AI 그래프를 보면,
Generative AI는 현재 '부풀려진 기대의 정점'에 있는 것으로 보인다.
곧 환멸의 단계에 이르른 후 다시 계몽 및 생산성 안정의 단계에 접어들 것으로 예상된다.

# 클라우드 vs 온프레미스
클라우드는 효율적인 자원 확장이 장점. 온프레미스는 물리적 데이터 보안이 장점이다.

클라우드의 장점은 다음과 같다.
자원 및 운용 활용성 증가
효율적인 자원 확장 용이
저렴한 초기 비용

클라우드의 단점은 다음과 같다.
지속적인 클라우드 운영 비용
데이터 유출 위험

온프레미스의 장점은 다음과 같다.
물리적 데이터 보안 가능
사용자에 맞춤 시스템 구축
저렴한 유지보수 비용

온프레미스의 단점은 다음과 같다.
높은 초기 구축 비용
지속적인 IT 인프라 관리 필요

# 프라이빗 클라우드(=하이브리드 클라우드)
퍼블릭 클라우드의 확장성과 온프레미스의 보안성을 결합한 것이다.
24년 하반기 출시될 아이폰 16에 탑재될 온디바이스 AI의 경우 프라이빗 클라우드로 보안성 강화

# 온디바이스 AI
온디바이스 AI는 인터넷 연결 없이도 AI가 스마트폰이나 PC 등 단말기 자체적으로 정보를 수집/연산하는 것이 가능하다.
'갤럭시 S24' 시리즈를 시작으로 온디바이스 AI 기능을 탑재한 IT 제품들 출시 중이다.

클라우드 AI는 데이터를 서버로 전송해 연산 작업을 거친 뒤, 결괏값을 단말기로 다시 받는 방식이다.
온디바이스 AI는 이와 달리 네트워크망을 통해 정보가 오고 가는 동안 데이터 전송 지연 등의 문제가 없어 번역 등 실시간 작업에 유리하다. 
또 개인정보 유출 등 보안 문제에 대한 우려도 적으며, 이에 따라 AI를 개인화하는 일도 가능해진다.
현재 생성형 AI 수준의 연산 능력을 온디바이스 AI로 구현하기 위해서는 수십 기가바이트(GB)에서 수백 GB 수준의 메모리 용량이 필요하다.
업계에 따르면 맥쿼리증권은 최근 보고서를 통해 스마트폰에 온다비아스 AI로 이미지 생성 기능을 구현하려면 최소 12GB 수준의 메모리가 필요하며, 디지털 AI 보조 기능을 갖추려면 20GB의 램이 필요할 것으로 내다봤다. 현재 일반 스마트폰에 채택된 D램(8GB) 대비 용량이 2배로 증가하는 것이다.

https://live.lge.co.kr/2312-lg-gram-pro-2024/
새 CPU에는 인텔 칩 가운데 최초로 인공지능 연산에 특화된 반도체 신경망처리장치(NPU) 인텔® AI Boost가 내장돼, 
네트워크 연결 없이도 자체 AI 연산이 가능하다. 예를 들어 사진을 분석해 인물/장소/날짜 등 38개 카테고리에 따라 자동으로 분류한다.

# sLLM(smaller Large Language Model)
온디바이스 AI에는 sLLM이 들어감
sLLM은 LLM 중에서도 모델 사이즈가 작은 것
 - Meta 오픈 소스 "Llama"의 경량화 버전은 매개변수가 7B(70억 개)
 - 스탠포드대학교는 llama 7B를 기반의 sLLM '알파카' 공개
 - 국내 기업 업스테이지는 llama 2를 파인튜닝한 'Solar' 모델을 만들어 허깅페이스 '오픈 LLM 리더보드' 1위 달성(2023.12)
sLLM은 LLM에 비해 더 적은 컴퓨팅 자원으로 최대한의 효율을 냄 
sLLM은 인터넷 연결이 필요 없음. 모바일·노트북 등 전자기기에 곧바로 적용할 수 있음
  -> '온디바이스 AI' 시장에서 sLLM이 각광받고 있는 이유

Solar는 Llama 파인튜닝을 통해 좋은 성능을 보임. 특히 한국어에 특화된 솔루션 제공에 강점

# 파인튜닝(Fine-tuning)
파인튜닝은 이미 훈련된 모델(Pre-trained Model)을 특정 task(ex. 번역)를 잘 하도록 재학습 시키는 것을 말한다.
파인튜닝은 전이학습의 일종이다.

# LoRA(Low Rank Adaptation)
효율적으로 파인튜닝(PEFT; Parameter Efficient Fine Tuning)하는 방법 중 하나 
기존 모델을 건드리지 않고, 추가된 일부 layer(LoRA layer)만 학습시킴
* LoRA(Low Rank Adaptation) : 저랭크 행렬을 추가하여 학습할 파라미터 수를 줄이는 방법.

# PEFT(Parameter Efficient Fine Tuning)의 종류
LoRA 말고도 다양한 peft 방법들이 존재

# PEFT(Parameter Efficient Fine Tuning)의 효율성
LoRA 방식으로 학습한 GPT3의 경우, 풀 파인튜닝된 모델과 성능이 비슷하거나 더 나음

# LoRA Fine-tuning 예시
Solar10.7B 모델에 인테리어 데이터를 LoRA 파인튜닝한 예시를 보면,
파인튜닝된 모델로 질문하는 파이썬 함수를 만들고, 이를 사용한다.
인테리어 관련 질문을 던지면 이에 맞는 답변을 한다.

# sLLM 한계는 다음과 같다.
1. 성능 저하
 - 모델에 따라 정확도가 감소할 수 있음
 - 긴 문맥을 이해하거나 복잡한 질문에 답변할 때 한계

2. 일반화 능력 제한 
 - 새로운 상황이나 데이터에 대한 일반화 능력이 제한적
 - 텍스트 or 이미지 or 동영상 등 단일 task에만 특화되어 멀티모달 기능은 아직 부족

3. 기능 제한
 - 멀티태스킹 능력이 제한적일 수 있으며, 특정 작업에서만 유용할 수 있음








